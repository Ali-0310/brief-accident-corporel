# ğŸš— Projet Accidents Corporels - Base de DonnÃ©es Analytique

> **Objectif** : Conception d'une base de donnÃ©es centralisÃ©e pour l'analyse des accidents corporels de la circulation routiÃ¨re en France (donnÃ©es BAAC 2012-2019)

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Contexte et Objectifs](#-contexte-et-objectifs)
2. [Architecture & Workflow Global](#-architecture--workflow-global)
3. [Workflow d'Ingestion](#-workflow-dingestion)
   - [Extraction depuis l'API](#1-extraction-depuis-lapi)
   - [Ingestion dans la couche Bronze](#2-ingestion-dans-la-couche-bronze)
   - [Construction de la couche Silver](#3-construction-de-la-couche-silver)
   - [Ingestion dans la couche Silver](#4-ingestion-dans-la-couche-silver)
4. [Construction de la Couche Gold](#-construction-de-la-couche-gold)
   - [Tables de Dimensions](#41-tables-de-dimensions)
   - [Tables de Faits](#42-tables-de-faits)
5. [Points d'AmÃ©lioration](#-points-damÃ©lioration)
6. [Installation et Usage](#-installation-et-usage)

---

## ğŸ¯ Contexte et Objectifs

### Mission

Concevoir une base de donnÃ©es pour un **observatoire rÃ©gional de la sÃ©curitÃ© routiÃ¨re**, permettant d'analyser les accidents corporels et d'identifier les facteurs de risque.

### Questions MÃ©tier ClÃ©s

1. **Zones Ã  risque** : Quelles communes/dÃ©partements ont le plus d'accidents graves ?
2. **Conditions critiques** : Quelles combinaisons (mÃ©tÃ©o + luminositÃ© + type de route) sont les plus dangereuses ?
3. **Tendances temporelles** : Y a-t-il des semaines anormales avec pics d'accidents ?
4. **Profils de victimes** : Quels types d'usagers sont les plus vulnÃ©rables ?

### Source de DonnÃ©es

- **API-Opendatasoft** : [MinistÃ¨re de l'IntÃ©rieur - Base Nationale des Accidents Corporels (BAAC)](https://public.opendatasoft.com/explore/assets/accidents-corporels-de-la-circulation-millesime/)
- **PÃ©riode** : 2012-2019
- **Format** : Parquet â†’ PostgreSQL
- **Volume** : **475k de lignes**

---

## ğŸ—ï¸ Architecture & Workflow Global

### DÃ©finition de l'Architecture

Nous avons adoptÃ© une **architecture Medallion en 3 couches** inspirÃ©e des pratiques Databricks et Data Lakes modernes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ARCHITECTURE MEDALLION                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        API Opendatasoft (Request - methode get)
           â”‚
           â”œâ”€â–º Parquet brut 
           â”‚
           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  COUCHE BRONZE   â”‚  â† DonnÃ©es brutes (staging)
   â”‚  (Staging Zone)  â”‚     - 1 table : raw_accidents
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - 69 colonnes TEXT
            â”‚                - Aucune transformation
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  COUCHE SILVER   â”‚  â† DonnÃ©es nettoyÃ©es et normalisÃ©es
   â”‚  (Snowflake 3NF) â”‚     - 4 tables : accidents, lieux, vehicules, usagers
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - Foreign Keys strictes
            â”‚                - Types validÃ©s (DATE, INTEGER, BOOLEAN)
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   COUCHE GOLD    â”‚  â† ModÃ¨le analytique optimisÃ©
   â”‚  (Constellation) â”‚     - 5 dimensions (date, gÃ©ographie, conditions, route,
   |                  |       vÃ©hicule)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - 3 faits (accidents, vÃ©hicules, usagers)
                             - MÃ©triques prÃ©-calculÃ©es
```
### SchÃ©mas : 
![Schema bronze](img/bronze.JPG "SchÃ©ma en bronze")
![Schema Gold](img/silver.JPG "SchÃ©ma en Silver")
![Schema Gold](img/Gold.JPG "SchÃ©ma en gold ")
### Principes de ModÃ©lisation

| Couche | ModÃ¨le | Objectif | Usage |
|--------|--------|----------|-------|
| **Bronze** | Table unique (staging) | TraÃ§abilitÃ© totale | ETL, rejeu des transformations |
| **Silver** | Snowflake (3NF) | IntÃ©gritÃ© rÃ©fÃ©rentielle | RequÃªtes transactionnelles, validation |
| **Gold** | Constellation | Performance analytique | Dashboards BI, requÃªtes OLAP |

### Workflow Global : RÃ©partition des TÃ¢ches

Notre Ã©quipe de 3 apprentis data engineers s'est rÃ©partie les tÃ¢ches selon ce workflow :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RÃ‰PARTITION DES TÃ‚CHES                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¥ INGESTION (Mathieu)
   â”œâ”€ Extraction API Opendatasoft
   â”œâ”€ Lecture du fichier Parquet
   â””â”€ Source code : notebook/etl_accidents.ipynb| notebook/ressources

ğŸ§¹ TRANSFORMATION SILVER (David)
   â”œâ”€ Nettoyage et validation des types
   â”œâ”€ Normalisation 3NF (4 tables)
   â”œâ”€ Gestion des FK et contraintes
   â””â”€ source code : Ã  dÃ©finir

â­ MODÃ‰LISATION GOLD (Ali-Mathieu)
   â”œâ”€ Conception du schÃ©ma constellation
   â”œâ”€ Construction de 4 dimensions de la couche Gold
   |                    (dim_condition,dim_vehicule,dim_date et dim condition)
   |
   â”œâ”€ Construction des 1 dimensions de la couche Gold (dim_geographie)
   â”œâ”€ Construction des 3 tables de faits (fait_accidents,fait_vehicules et
   |                                       fait usagers)
   â”œâ”€ Scripts SQL : sql/
   â””â”€ Script Python : notebook/etl_accidents.ipynb| notebook/ressources | 
                        notebook/transformation_gold.ipynb

ğŸ“Š VALIDATION & DOCUMENTATION (Tous)
   â”œâ”€ Tests de cohÃ©rence des donnÃ©es
   â”œâ”€ RequÃªtes mÃ©tier
   â””â”€ CrÃ©ation de la Documentation
```

---

## ğŸ”„ Workflow d'Ingestion

### 1. Extraction depuis l'API

**Responsable** : Mathieu

**Source** : API Opendatasoft - Base BAAC (Bulletins dâ€™Analyse des Accidents Corporels)

**Processus** :</br>
* Pour lâ€™extraction des donnÃ©es nous avons utilisÃ© le package Python `requests` pour faire une requÃªte HTTP `GET` sur lâ€™endpoint `/catalog/datasets/{dataset_id}/exports/parquet`.

>* **A noter** : les politiques de contrÃ´le de dÃ©bit en place sur lâ€™API dâ€™OpenData ne nous ont permis de rÃ©cupÃ©rer le fichier quâ€™en un seul coup. CÃ´tÃ© format de fichier, nous avons optÃ© pour lâ€™utilisation du format parquet pour optimiser les temps de traitement.


**Output** :
- Fichier : `accidents_raw.parquet`
- Volume : ~475911 de lignes Ã— 69 colonnes

**Script** : `notebook/etl_accidents.ipynb` *(gÃ©rÃ© par collÃ¨gue 1)*

---

### 2. Ingestion dans la Couche Bronze

* **Responsable** : Mathieu

* **Objectif** : Charger les donnÃ©es brutes dans PostgreSQL sans transformation, pour traÃ§abilitÃ©.
* **Explication** : Une fois le dataset extrait, on utilise le package `SQLAlchemy` de python pour pouvoir initier lâ€™ingestion des donnÃ©es dans une seule et unique table brute, le but Ã©tant par la suite dâ€™utiliser cette table pour pouvoir rÃ©cupÃ©rer les colonnes nÃ©cessaires Ã  la construction de chaque table des couches suivants Silver et Gold.

**Structure Bronze** :

```sql
-- Schema : accidents_bronze
-- Table : raw_accidents

CREATE TABLE accidents_bronze.raw_accidents (
    row_id BIGSERIAL PRIMARY KEY,           -- ClÃ© technique
    load_timestamp TIMESTAMP DEFAULT NOW(), -- Horodatage du chargement
    
    -- 84 colonnes en type TEXT (donnÃ©es brutes)
    num_acc TEXT,
    jour TEXT,
    mois TEXT,
    an TEXT,
    hrmn TEXT,
    lum TEXT,
    dep TEXT,
    com TEXT,
    -- ... (80 autres colonnes)
);
```
---

### 3. Construction de la Couche Silver

**Responsable** : David

* **Objectif** : Normaliser les donnÃ©es en modÃ¨le 3NF (Flocon) avec validation des types et contraintes.

* **Explication** : La couche Silver constitue la premiÃ¨re Ã©tape de nettoyage et de normalisation des donnÃ©es brutes.Ã€ partir de la table unique de la couche Bronze, nous avons appliquÃ© un processus ETL optimisÃ© pour transformer les donnÃ©es en 4 tables relationnelles.

**Structure Silver** : 4 tables normalisÃ©es

#### Table CrÃ©Ã©es: 

* `accidents`**(Table Centrale)** : Informations gÃ©nÃ©rales sur chaque accident (temporalitÃ©, localisation, conditions) 
    * **Grain** : 1 ligne = 1 accident corporel

* `lieux` **(relation 1:1 avec accidents)** : CaractÃ©ristiques dÃ©taillÃ©es du lieu (type de route, infrastructure, surface) 
    * **Grain** : 1 ligne = 1 lieu

* `vehicules` **(relation N:1 avec accidents)**: DonnÃ©es sur les vÃ©hicules impliquÃ©s (catÃ©gorie, manÅ“uvre, obstacles) 
    * **Grain** : 1 ligne = 1 vÃ©hicule

* `usagers`**(relation N:1 avec vehicules)** : Informations sur les personnes impliquÃ©es (Ã¢ge, gravitÃ©, Ã©quipement de sÃ©curitÃ©)
    * **Grain** : 1 ligne = 1 usager


#### Transformations appliquÃ©es 

* **Explosion des colonnes multi-valeurs** : Transformation vectorisÃ©e des donnÃ©es agrÃ©gÃ©es (un accident â†’ plusieurs vÃ©hicules/usagers)

* **Normalisation des donnÃ©es** : Application de mappings pour standardiser les valeurs catÃ©gorielles

* **Nettoyage** : Gestion des valeurs manquantes, aberrantes et des formats inconsistants

* **Typage fort**: Conversion en types de donnÃ©es appropriÃ©s (Int64, Float)

#### Optimisations techniques 

* **Vectorisation complÃ¨te :** OpÃ©rations vectorisÃ©es pandas

* **Chargement sÃ©lectif :** Lecture uniquement des colonnes nÃ©cessaires 

* **Traitement par chunks :** Gestion optimisÃ©e de la mÃ©moire pour les gros volumes


### 4. Ingestion dans la Couche Silver

**Responsable** : David

**Explication** : Une fois les transformations effectuÃ©es, les 4 DataFrames sont chargÃ©s dans PostgreSQL via `SQLAlchemy` avec les optimisations suivantes :

**Processus d'ingestion** :

1. **Vidage des tables existantes :** TRUNCATE CASCADE pour garantir la fraÃ®cheur des donnÃ©es
2. **Insertion par batches :** Chargement optimisÃ© par lots de 5000 lignes (method='multi')
3. **Gestion des erreurs :** Try/catch avec rollback automatique en cas d'Ã©chec

**Configuration** :
- **Moteur** : SQLAlchemy avec pool de connexions (pool_pre_ping=True)
- **Format** : Insertion directe depuis pandas (to_sql)
- **Logging** : TraÃ§abilitÃ© complÃ¨te des opÃ©rations dans etl_logs.txt

---

## â­ Construction de la Couche Gold

Les tables Gold ont Ã©tÃ© crÃ©Ã©es via les scripts du dossier `sql/03_Gold/`.
La prochaine Ã©tape a donc Ã©tÃ© de transformer les donnÃ©es et de les insÃ©rer dans ces tables.

### Transformation/Chargement des tables : Partie 1

**Responsable** : Ali

**Description** : Construction de la dimension gÃ©ographie et des 3 tables de faits (fait_accidents, fait_vehicules, fait_usagers).

### Transformation/Chargement des tables : Partie 2

**Responsable** : Mathieu

Cette partie a Ã©tÃ© traitÃ©e par Mathieu, notamment pour les tables suivantes :
- `dim_conditions`
- `dim_vehicule`
- `dim_route`
- `dim_date`

Pour cette partie, des requÃªtes SELECT sont utilisÃ©es afin de rÃ©cupÃ©rer les colonnes nÃ©cessaires au remplissage des nouvelles tables Gold, aprÃ¨s avoir effectuÃ© de multiples transformations sur les donnÃ©es en question.

Afin de limiter la quantitÃ© de code dans les cellules, des fichiers Python et JSON ont Ã©tÃ© utilisÃ©s pour Ã©tablir des mappings entre libellÃ©s de valeur et code valeur ; le but Ã©tant de remplir les nouvelles colonnes qui n'existaient pas dans la table RAW de la couche Bronze.

---

## ğŸ“Š Analyses MÃ©tier

Une fois les tables Gold constituÃ©es, nous avons dÃ©veloppÃ© **3 analyses mÃ©tier clÃ©s** pour extraire des insights actionnables sur la sÃ©curitÃ© routiÃ¨re. Ces analyses rÃ©pondent Ã  des questions stratÃ©giques en matiÃ¨re de prÃ©vention.

### Stack Technique

- **Moteur de requÃªtes** : PostgreSQL avec requÃªtes SQL optimisÃ©es (CTE, Window Functions)
- **Orchestration** : Python (SQLAlchemy, Pandas)
- **Configuration** : Variables d'environnement (.env) pour sÃ©curitÃ©
- **Logging** : TraÃ§abilitÃ© complÃ¨te des analyses avec horodatage
- **Dictionnaires de rÃ©fÃ©rence** : Utilisation de mappings Python pour transformer les codes numÃ©riques en libellÃ©s mÃ©tier lisibles (conditions atmosphÃ©riques, luminositÃ©, gravitÃ©, etc.)

### CritÃ¨re 1 : Conditions Ã  Risque Ã‰levÃ©

**Question mÃ©tier** :
Quelles combinaisons de conditions (mÃ©tÃ©o Ã— luminositÃ© Ã— type de route Ã— Ã©tat de surface) prÃ©sentent un risque significativement supÃ©rieur Ã  la moyenne nationale ?

**MÃ©thodologie** :
- Calcul du taux de mortalitÃ© national comme rÃ©fÃ©rence (base 100)
- AgrÃ©gation par combinaisons de 4 facteurs (conditions atmosphÃ©riques, luminositÃ©, catÃ©gorie de route, Ã©tat de surface)
- Filtrage sur seuil de significativitÃ© : minimum 50 accidents par combinaison
- Calcul du ratio de surmortalitÃ© : comparaison au taux national
- Classification des risques :
  - **TRÃˆS DANGEREUX** : taux > 3Ã— la moyenne nationale
  - **DANGEREUX** : taux > 2Ã— la moyenne
  - **RISQUE Ã‰LEVÃ‰** : taux > 1.5Ã— la moyenne

**Output** : Top 20 des combinaisons les plus dangereuses avec scoring de risque

### CritÃ¨re 2 : Zones FrÃ©quentÃ©es vs Zones Dangereuses

**Question mÃ©tier** :
Les zones avec beaucoup d'accidents sont-elles rÃ©ellement plus dangereuses, ou simplement plus frÃ©quentÃ©es ? Comment identifier les zones prioritaires pour les actions de prÃ©vention ?

**MÃ©thodologie** :
- Calcul de 2 indicateurs par dÃ©partement :
  - **Volume** : nombre total d'accidents
  - **GravitÃ©** : taux d'accidents graves (tuÃ©s + blessÃ©s hospitalisÃ©s)
- Segmentation en 4 quadrants via analyse quartiles (Q2, Q3)

**Typologie des zones** :
- **Zone TRÃˆS PRIORITAIRE** : Fort volume + Forte gravitÃ©
- **Zone DANGEREUSE** : Faible volume + Forte gravitÃ©  
- **Zone FRÃ‰QUENTÃ‰E** : Fort volume + Faible gravitÃ©
- **Zone SOUS SURVEILLANCE** : Faible volume + Faible gravitÃ©
![Typologie des zones ](img/typo_des_zones.JPG "typologie des zone")

**Output** : Classification de tous les dÃ©partements avec double classement (rang volume + rang gravitÃ©)

### CritÃ¨re 3 : Analyse par CatÃ©gorie d'Usagers

**Question mÃ©tier** :
Quelles catÃ©gories d'usagers sont les plus vulnÃ©rables ? Quel est leur niveau de surrisque par rapport Ã  la moyenne ?

**MÃ©thodologie** :
- Calcul du taux de mortalitÃ© global (tous usagers confondus)
- Statistiques dÃ©taillÃ©es par catÃ©gorie d'usager :
  - Nombre total d'usagers impliquÃ©s
  - DÃ©compte par gravitÃ© (tuÃ©s, blessÃ©s hospitalisÃ©s, blessÃ©s lÃ©gers, indemnes)
  - Taux de mortalitÃ© et taux de gravitÃ©
  - Ratio de vulnÃ©rabilitÃ© : comparaison au taux global
- Seuil de significativitÃ© : minimum 100 usagers par catÃ©gorie
- Classification de vulnÃ©rabilitÃ© :
  - **TRÃˆS VULNÃ‰RABLE** : taux > 3Ã— la moyenne globale
  - **VULNÃ‰RABLE** : taux > 2Ã— la moyenne
  - **Ã€ RISQUE** : taux > 1.5Ã— la moyenne

**Output** : Classement des catÃ©gories par taux de mortalitÃ© avec scoring de vulnÃ©rabilitÃ©

---

## ğŸš€ Points d'AmÃ©lioration

- **Orchestration du pipeline ETL avec Apache Airflow** : automatisation des flux Bronze â†’ Silver â†’ Gold, planification quotidienne et monitoring visuel.

- **Optimisation des performances PostgreSQL** : partitionnement par annÃ©e, index composites et vues matÃ©rialisÃ©es pour accÃ©lÃ©rer les requÃªtes.

