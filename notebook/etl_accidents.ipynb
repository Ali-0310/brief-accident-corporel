{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MNhU1RsZB0hy",
   "metadata": {
    "id": "MNhU1RsZB0hy"
   },
   "source": [
    "# ETL Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e618a14",
   "metadata": {
    "id": "5e618a14"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "db = os.getenv(\"POSTGRES_DB\")\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e8e6e-19f1-4bcc-93d3-9e108e46945b",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "## 1 - Extraction parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0982aa-bef1-4d9b-bdbb-b8f50826b08f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkT5pZCFpeLW",
    "outputId": "f928a65c-ccdc-4fd0-8b5a-d3e23f83b9f0"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL Parquet de ton dataset\n",
    "dataset_id = \"accidents-corporels-de-la-circulation-millesime\"\n",
    "parquet_url = f\"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/{dataset_id}/exports/parquet\"\n",
    "\n",
    "# T√©l√©charger le Parquet directement\n",
    "response = requests.get(parquet_url, timeout=60)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Erreur t√©l√©chargement Parquet: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f1de096-3ab3-4c73-853e-70ebd4fa0a17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkT5pZCFpeLW",
    "outputId": "f928a65c-ccdc-4fd0-8b5a-d3e23f83b9f0"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "# Lire le contenu Parquet en DataFrame directement depuis le flux m√©moire\n",
    "parquet_data = response.content\n",
    "raw_dataframe = pd.read_parquet(io.BytesIO(parquet_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad4f38-30d0-422d-ac26-9dc90ecdad7c",
   "metadata": {
    "id": "V_qz1dbYBo_G",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extraction Pagin√©e: Non utilis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccdc22a-e5af-499e-b82b-12cbe1cd03dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkT5pZCFpeLW",
    "outputId": "f928a65c-ccdc-4fd0-8b5a-d3e23f83b9f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Pagination API des accidents corporels...\n",
      "üîπ Chunk r√©cup√©r√©: 10000 lignes (offset 0)\n",
      "‚ùå Erreur API: 400\n",
      "Nombre de chunks r√©cup√©r√©s : 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_accidents_paginated(limit=500):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les donn√©es de l'API OpenDataSoft en JSON,\n",
    "    en utilisant une pagination via 'start' et 'rows'.\n",
    "    Chaque chunk est stock√© dans une liste de DataFrames.\n",
    "    G√®re automatiquement le rate limit (429) avec pause.\n",
    "    \"\"\"\n",
    "    url = \"https://public.opendatasoft.com/api/records/1.0/search/\"\n",
    "    dataset_slug = \"accidents-corporels-de-la-circulation-millesime@public\"\n",
    "\n",
    "    offset = 0\n",
    "    all_chunks = []\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset_slug,\n",
    "            \"rows\": limit,       # nombre de lignes par chunk\n",
    "            \"start\": offset,     # offset pour pagination\n",
    "            \"timezone\": \"Europe/Paris\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "\n",
    "            # Gestion du rate limit\n",
    "            if response.status_code == 429:\n",
    "                print(\"‚è≥ Rate limit atteint, pause 10 secondes...\")\n",
    "                time.sleep(10)\n",
    "                continue  # recommence la m√™me requ√™te\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Erreur API: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            data = response.json().get(\"records\", [])\n",
    "            if not data:\n",
    "                print(\"üîπ Plus de donn√©es √† r√©cup√©rer.\")\n",
    "                break\n",
    "\n",
    "            # Convertir le chunk JSON en DataFrame\n",
    "            df_chunk = pd.json_normalize(data)\n",
    "            all_chunks.append(df_chunk)\n",
    "\n",
    "            print(f\"üîπ Chunk r√©cup√©r√©: {len(df_chunk)} lignes (offset {offset})\")\n",
    "\n",
    "            # Pause courte pour √©viter le rebuke du serveur\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Avancer l'offset pour le prochain chunk\n",
    "            offset += len(df_chunk)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de la r√©cup√©ration du chunk: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "# Utilisation\n",
    "print(\"üîç Pagination API des accidents corporels...\")\n",
    "chunks_list = fetch_accidents_paginated(limit=10000)\n",
    "\n",
    "print(\"Nombre de chunks r√©cup√©r√©s :\", len(chunks_list))\n",
    "\n",
    "# Optionnel : concat√©ner tous les chunks en un seul DataFrame\n",
    "# df_accidents = pd.concat(chunks_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5f613-b079-4826-bd7f-2509aeb40c92",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "# 2 - Chargement donn√©es brutes dans table RAW - couche Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b961699-d5e0-43c5-af90-27cd778d6e7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkT5pZCFpeLW",
    "outputId": "f928a65c-ccdc-4fd0-8b5a-d3e23f83b9f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connexion PostgreSQL\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")\n",
    "\n",
    "# Sauvegarder localement en fichier Parquet (optionnel)\n",
    "raw_dataframe.to_parquet(\"accidents.parquet\", index=False)\n",
    "\n",
    "# Nettoyage\n",
    "if \"datetime\" in raw_dataframe.columns:\n",
    "    raw_dataframe.drop(columns=[\"datetime\"], inplace=True)\n",
    "if \"int\" in raw_dataframe.columns:\n",
    "    raw_dataframe.rename(columns={\"int\": \"intsect\"}, inplace=True)\n",
    "\n",
    "# Lecture chunk par chunk pour insertion PostgreSQL\n",
    "chunksize = 50000\n",
    "for start in range(0, len(raw_dataframe), chunksize):\n",
    "    chunk = raw_dataframe.iloc[start:start+chunksize]\n",
    "    chunk.to_sql(\"raw_accidents\", engine, schema=\"accidents_bronze\", if_exists=\"append\", index=False)\n",
    "    print(f\"Chunk {start//chunksize + 1} ins√©r√© ({len(chunk)} lignes)\")\n",
    "\n",
    "print(\"‚úÖ Conversion CSV ‚Üí Parquet et insertion termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7be5d-a1b0-442c-910b-a3da823f4ca7",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "# 3 - Transformation et insertion - couche Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f13aa48-c090-4744-ab36-ce29bec6199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utiliser si la table raw est d√©j√† remplie et la connexion n'est plus en m√©moire\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204d744-1675-4b76-9034-e17dc7f82ffe",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "### 3.1 - Transformation et insertion: Tables dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c87480-0a32-4da1-abc7-acaf0fd52f3f",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "#### 3.1.1 - Table dim_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2c692-66b1-45ec-a45f-5cce8f5df667",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "S√©paration des op√©rations de chargement de fichiers et d√©finition des fonctions outils dans le but d'all√©ger les cellules de transformation / chargement.\n",
    "Les fichiers JSON servent √† trouver la correspondance entre des cl√©s faisant r√©f√©rence √† une donn√©e, donc ici, les donn√©es m√©t√©orologiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1615260-071a-48ff-822e-95ce11f9bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Outils dim_conditions\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Charger les JSON\n",
    "with Path(\"./resources/cond_meteo.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    conditions_data = json.load(f)[\"conditions_meteo\"]\n",
    "with Path(\"./resources/cond_lum.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    luminosite_data = json.load(f)[\"conditions_luminosite\"]\n",
    "\n",
    "# Cr√©er un dictionnaire index√© par code pour acc√®s rapide\n",
    "luminosite_dict = {cond[\"code\"]: cond for cond in luminosite_data}\n",
    "\n",
    "# Cr√©er un dictionnaire index√© par code pour acc√®s rapide\n",
    "conditions_dict = {cond[\"code\"]: cond for cond in conditions_data}\n",
    "\n",
    "def get_condition_value(code: str, key: str):\n",
    "    condition = conditions_dict.get(code)\n",
    "    if condition:\n",
    "        return condition.get(key)\n",
    "    return None\n",
    "\n",
    "def normalize_libelle(libelle):\n",
    "    if libelle is None:\n",
    "        return \"\"\n",
    "    return str(libelle).strip().lower().replace(\"\\u00A0\", \" \")\n",
    "\n",
    "def get_luminosite_code_from_libelle(libelle: str):\n",
    "    lib_norm = normalize_libelle(libelle)\n",
    "    for code, data in luminosite_dict.items():\n",
    "        json_lib_norm = normalize_libelle(data.get(\"libelle\"))\n",
    "        if lib_norm == json_lib_norm:\n",
    "            return int(code)\n",
    "    return None\n",
    "\n",
    "def get_condition_code_from_libelle(libelle: str):\n",
    "    lib_norm = normalize_libelle(libelle)\n",
    "    for code, data in conditions_dict.items():\n",
    "        json_lib_norm = normalize_libelle(data.get(\"libelle\"))\n",
    "        if lib_norm == json_lib_norm:\n",
    "            return int(code)\n",
    "    return None\n",
    "\n",
    "def calcul_risque(row):\n",
    "    est_nuit = row[\"est_nuit\"]\n",
    "    est_intemperie = row[\"est_intemperie\"]\n",
    "\n",
    "    if not est_nuit and not est_intemperie:\n",
    "        return 1  # Plein jour, pas d'intemp√©ries\n",
    "    elif est_nuit and est_intemperie:\n",
    "        return 5  # Nuit + intemp√©ries\n",
    "    elif est_nuit or est_intemperie:\n",
    "        return 4  # Nuit ou intemp√©ries\n",
    "    else:\n",
    "        return 2  # Cas interm√©diaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e40676-ad33-4fce-8783-5a44d364eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Transform/Load\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "# DataFrame vide\n",
    "df_dim_conditions = pd.DataFrame(columns=[\n",
    "    \"luminosite_code\",\n",
    "    \"luminosite_libelle\",\n",
    "    \"est_nuit\",\n",
    "    \"atm_code\",\n",
    "    \"atm_libelle\",\n",
    "    \"est_intemperie\",\n",
    "    \"niveau_risque\"\n",
    "])\n",
    "\n",
    "# Flush de la table avant insertion (WORKFLOW DE TEST)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"TRUNCATE TABLE accidents_gold.dim_conditions RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "##################################\n",
    "# Lecture et conversion tol√©rante\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT lum, atm\n",
    "        FROM accidents_bronze.raw_accidents\n",
    "        WHERE lum IS NOT NULL AND atm IS NOT NULL\n",
    "    \"\"\"))\n",
    "\n",
    "    for row in result.mappings():\n",
    "        lum_raw = row[\"lum\"]\n",
    "        atm_raw = row[\"atm\"]\n",
    "\n",
    "        # Conversion tol√©rante : texte ou nombre\n",
    "        try:\n",
    "            lum_code = int(lum_raw)\n",
    "        except (ValueError, TypeError):\n",
    "            # Chercher le code correspondant au libell√©\n",
    "            lum_code = next((k for k,v in luminosite_dict.items() if v.get(\"libelle\") == lum_raw), None)\n",
    "\n",
    "        try:\n",
    "            atm_code = int(atm_raw)\n",
    "        except (ValueError, TypeError):\n",
    "            atm_code = next((k for k,v in conditions_dict.items() if v.get(\"libelle\") == atm_raw), None)\n",
    "\n",
    "        # Sauter les valeurs non reconnues\n",
    "        if lum_code is None or atm_code is None:\n",
    "            continue\n",
    "\n",
    "        # R√©cup√©rer libell√©s et bool√©ens\n",
    "        est_nuit_val = bool(luminosite_dict.get(lum_code, {}).get(\"est_nuit\", False))\n",
    "        est_intemp_val = bool(conditions_dict.get(atm_code, {}).get(\"est_intemperie\", False))\n",
    "        lum_libelle_val = luminosite_dict.get(lum_code, {}).get(\"libelle\", f\"Code {lum_code}\")\n",
    "        atm_libelle_val = conditions_dict.get(atm_code, {}).get(\"libelle\", f\"Code {atm_code}\")\n",
    "\n",
    "        # On supprime les duplicats de cl√© composite\n",
    "        df_dim_conditions.drop_duplicates(subset=['luminosite_code', 'atm_code'], keep='first', inplace=True)\n",
    "        \n",
    "        df_dim_conditions.loc[len(df_dim_conditions)] = [\n",
    "            lum_code,\n",
    "            lum_libelle_val,\n",
    "            est_nuit_val,\n",
    "            atm_code,\n",
    "            atm_libelle_val,\n",
    "            est_intemp_val,\n",
    "            calcul_risque({\"est_nuit\": est_nuit_val, \"est_intemperie\": est_intemp_val})\n",
    "        ]\n",
    "\n",
    "# S'assurer que les bool√©ens sont bien bool\n",
    "df_dim_conditions[\"est_nuit\"] = df_dim_conditions[\"est_nuit\"].fillna(False).astype(bool)\n",
    "df_dim_conditions[\"est_intemperie\"] = df_dim_conditions[\"est_intemperie\"].fillna(False).astype(bool)\n",
    "\n",
    "# Insertion en batch dans la table gold\n",
    "\n",
    "# Insertion rapide dans PostgreSQL avec pandas.to_sql\n",
    "# Si la table existe d√©j√† et a la contrainte UNIQUE, on peut g√©rer le conflit via 'if_exists' et 'method'\n",
    "df_dim_conditions.to_sql(\n",
    "    name='dim_conditions',\n",
    "    con=engine,\n",
    "    schema='accidents_gold',\n",
    "    if_exists='append',  # ajoute les lignes\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc4dbb-671c-4c9b-a9e4-458a076ce84f",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "#### 3.1.2 - Table dim_vehicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed303f39-b439-4364-a193-f20d57816bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Outils dim_vehicule\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemin vers ton fichier JSON\n",
    "json_file = Path(\"./resources/cat_veh.json\")\n",
    "\n",
    "# Charger le JSON dans un dictionnaire Python\n",
    "with json_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    vehicules_data = json.load(f)[\"categories\"]\n",
    "\n",
    "# Cr√©er un dictionnaire index√© par code pour acc√®s rapide\n",
    "vehicules_dict = {veh[\"code\"]: veh for veh in vehicules_data}\n",
    "\n",
    "def get_vehicule_code_by_label(label):\n",
    "    label_norm = label.strip().lower()\n",
    "    for code, veh in vehicules_dict.items():\n",
    "        libelle = veh.get(\"libelle\")\n",
    "        if libelle and libelle.strip().lower() == label_norm:\n",
    "            return code  # retourne la cl√© (code) sous forme de cha√Æne\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d26616-d287-4dbc-bff3-49e279e7790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Transform/Load\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionnaire de conversion des niveaux de protection en entiers\n",
    "niveau_protection_map = {\n",
    "    \"Faible\": 1,\n",
    "    \"Interm√©diaire\": 2,\n",
    "    \"√âlev√©e\": 3,\n",
    "    \"Elev√©e\": 3  # pour g√©rer absence d'accent si besoin\n",
    "}\n",
    "\n",
    "# Flush de la table avant insertion\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"TRUNCATE TABLE accidents_gold.dim_vehicule RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "# R√©cup√©ration des donn√©es depuis la base brute\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT catv\n",
    "        FROM accidents_bronze.raw_accidents\n",
    "        WHERE catv IS NOT NULL\n",
    "    \"\"\"))\n",
    "    temp_df = pd.DataFrame(result.fetchall(), columns=['catv'])\n",
    "\n",
    "# S√©parer les valeurs multiples et exploser en lignes\n",
    "temp_df['catv'] = temp_df['catv'].astype(str).apply(lambda x: x.split(',') if pd.notna(x) else [])\n",
    "temp_df = temp_df.explode('catv').reset_index(drop=True)\n",
    "\n",
    "# Convertir les libell√©s en codes en utilisant la fonction d√©di√©e\n",
    "temp_df['catv_code'] = temp_df['catv'].apply(get_vehicule_code_by_label)\n",
    "\n",
    "# Nettoyage des doublons √©ventuels avant cr√©ation du DF final\n",
    "# et suppression des lignes sans code\n",
    "temp_df = temp_df[temp_df['catv_code'].notna()]\n",
    "temp_df.drop_duplicates(subset=['catv_code'], inplace=True)\n",
    "\n",
    "# Construire ton DataFrame final avec codes et conversion de niveau_protection\n",
    "df_dim_vehicule = pd.DataFrame({\n",
    "    \"categorie_code\": temp_df['catv_code'],\n",
    "    \"categorie_libelle\": temp_df['catv'],\n",
    "    \"type_vehicule\": temp_df['catv_code'].astype(str).apply(lambda x: get_vehicule_value(x, \"type\")),\n",
    "    \"est_motorise\": temp_df['catv_code'].astype(str).apply(lambda x: get_vehicule_value(x, \"libelle\") != \"Bicyclette\"),\n",
    "    \"niveau_protection\": temp_df['catv_code'].astype(str).apply(lambda x: get_vehicule_value(x, \"niveau_protection\"))\n",
    "})\n",
    "\n",
    "# Appliquer la conversion des libell√©s 'niveau_protection' en entiers\n",
    "df_dim_vehicule['niveau_protection'] = df_dim_vehicule['niveau_protection'].map(niveau_protection_map)\n",
    "\n",
    "# Insertion en base via to_sql\n",
    "df_dim_vehicule.to_sql(\n",
    "    name='dim_vehicule',\n",
    "    con=engine,\n",
    "    schema='accidents_gold',\n",
    "    if_exists='append',\n",
    "    index=False,\n",
    "    method='multi'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d79792-1834-4df9-9cad-84cba2f6e4ef",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "#### 3.1.3- Table dim_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a84f4-7e6f-45c4-88ce-40f4df7e8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Transform/Load\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Import des fonctions de r√©f√©rence\n",
    "from refs_routes import (\n",
    "    get_trace_plan_df,\n",
    "    get_categorie_route_df,\n",
    "    get_profil_route_df,\n",
    "    get_etat_surface_df\n",
    ")\n",
    "\n",
    "# Initialisation du DataFrame\n",
    "df_dim_route = pd.DataFrame(columns=[\n",
    "    \"categorie_route_code\",\n",
    "    \"categorie_route_libelle\",\n",
    "    \"profil_route_code\",\n",
    "    \"profil_route_libelle\",\n",
    "    \"trace_plan_code\",\n",
    "    \"trace_plan_libelle\",\n",
    "    \"etat_surface_code\",\n",
    "    \"etat_surface_libelle\",\n",
    "    \"niveau_risque_route\"\n",
    "])\n",
    "\n",
    "# Fonction calcul_risque mise √† jour\n",
    "def calcul_risque(row):\n",
    "    if row[\"surf\"] == 7:\n",
    "        return 5  # Risque maximum : verglac√©\n",
    "    elif row[\"prof\"] == 3 and row[\"plan\"] == 4:\n",
    "        return 4  # Risque √©lev√© : sommet + virage en S\n",
    "    elif row[\"surf\"] in (2, 3, 4, 5, 6):\n",
    "        return 3  # Risque interm√©diaire : surfaces d√©grad√©es\n",
    "    elif row[\"surf\"] == 1 and row[\"prof\"] in (2, 4):\n",
    "        return 2  # Risque faible : conditions correctes mais pas parfaites\n",
    "    else:\n",
    "        return 1  # Normal : plat et route normale\n",
    "\n",
    "\n",
    "# R√©cup√©rer des codes √† partir des libell√©s\n",
    "def get_code_from_df(df, libelle, libelle_col=\"libelle\", code_col=\"code\"):\n",
    "    libelle_str = str(libelle).strip()\n",
    "    df[libelle_col] = df[libelle_col].astype(str).str.strip()\n",
    "    result = df.loc[df[libelle_col] == libelle_str, code_col]\n",
    "    return result.iloc[0] if not result.empty else None\n",
    "\n",
    "# Flush de la table avant insertion\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"TRUNCATE TABLE accidents_gold.dim_route RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "# Connexion et r√©cup√©ration des donn√©es\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT catr, prof, plan, surf\n",
    "        FROM accidents_bronze.raw_accidents\n",
    "        WHERE catr IS NOT NULL\n",
    "    \"\"\")).mappings()\n",
    "\n",
    "    # DataFrames de r√©f√©rence\n",
    "    df_trace_plan = get_trace_plan_df()\n",
    "    df_categorie_route = get_categorie_route_df()\n",
    "    df_profil_route = get_profil_route_df()\n",
    "    df_etat_surface = get_etat_surface_df()\n",
    "\n",
    "    # Construction du DataFrame\n",
    "    for row in result.fetchall():\n",
    "        df_dim_route.loc[len(df_dim_route)] = [\n",
    "            get_code_from_df(df_categorie_route, row[\"catr\"]),\n",
    "            row[\"catr\"],\n",
    "            get_code_from_df(df_profil_route, row[\"prof\"]),\n",
    "            row[\"prof\"],\n",
    "            get_code_from_df(df_trace_plan, row[\"plan\"]),\n",
    "            row[\"plan\"],\n",
    "            get_code_from_df(df_etat_surface, row[\"surf\"]),\n",
    "            row[\"surf\"],\n",
    "            calcul_risque(row)\n",
    "        ]\n",
    "\n",
    "df_dim_route.to_sql(\n",
    "    name='dim_route',\n",
    "    con=engine,\n",
    "    schema='accidents_gold',\n",
    "    if_exists='append',  # ajoute les lignes\n",
    "    index=False,\n",
    "    method='multi'       # ins√®re par lots, beaucoup plus rapide\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5688e-5135-48e9-8c72-a51c775b3203",
   "metadata": {
    "id": "V_qz1dbYBo_G"
   },
   "source": [
    "#### 3.1.4 - Table dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cc93e-c099-4e8a-a883-42c409b6c4ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Transform/Load\n",
    "\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from sqlalchemy import text\n",
    "import locale\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "def upsert_postgres(table, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    insert_stmt = insert(table.table).values(data)\n",
    "    upsert_stmt = insert_stmt.on_conflict_do_nothing(index_elements=['date_id'])\n",
    "    conn.execute(upsert_stmt)\n",
    "    \n",
    "# DataFrame vide\n",
    "df_dim_date = pd.DataFrame(columns=[\n",
    "    \"date_id\",\n",
    "    \"date_complete\",\n",
    "    \"annee\",\n",
    "    \"mois\",\n",
    "    \"jour\",\n",
    "    \"trimestre\",\n",
    "    \"jour_semaine\",\n",
    "    \"nom_jour\",\n",
    "    \"nom_mois\",\n",
    "    \"semaine_annee\",\n",
    "    \"jour_annee\",\n",
    "    \"est_weekend\",\n",
    "    \"est_jour_ferie\",\n",
    "    \"nom_jour_ferie\",\n",
    "    \"saison\"\n",
    "])\n",
    "\n",
    "def get_saison(mois):\n",
    "    if mois in [12, 1, 2]:\n",
    "        return \"Hiver\"\n",
    "    elif mois in [3, 4, 5]:\n",
    "        return \"Printemps\"\n",
    "    elif mois in [6, 7, 8]:\n",
    "        return \"√ât√©\"\n",
    "    else:\n",
    "        return \"Automne\"\n",
    "\n",
    "# Flush de la table avant insertion\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"TRUNCATE TABLE accidents_gold.dim_date RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "# Connexion et r√©cup√©ration des dates depuis la table raw\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT an AS annee, mois, jour\n",
    "        FROM accidents_bronze.raw_accidents\n",
    "        WHERE an IS NOT NULL AND mois IS NOT NULL AND jour IS NOT NULL\n",
    "    \"\"\")).mappings()\n",
    "\n",
    "    fr_holidays = holidays.France(years=[2025])  # adapter l'ann√©e si besoin\n",
    "\n",
    "    for row in result.fetchall():\n",
    "        annee = row[\"annee\"]\n",
    "        mois = row[\"mois\"]\n",
    "        jour = row[\"jour\"]\n",
    "        date_complete = pd.Timestamp(year=int(annee), month=int(mois), day=int(jour))\n",
    "\n",
    "        jour_semaine = date_complete.isoweekday()  # 1=Lundi, 7=Dimanche\n",
    "        est_weekend = jour_semaine in [6,7]\n",
    "        est_jour_ferie = date_complete in fr_holidays\n",
    "        nom_jour_ferie = fr_holidays.get(date_complete) if est_jour_ferie else None\n",
    "\n",
    "        df_dim_date.loc[len(df_dim_date)] = [\n",
    "            int(date_complete.strftime(\"%Y%m%d\")),  # date_id\n",
    "            date_complete,\n",
    "            annee,\n",
    "            mois,\n",
    "            jour,\n",
    "            date_complete.quarter,\n",
    "            jour_semaine,\n",
    "            date_complete.day_name(),\n",
    "            date_complete.month_name(),\n",
    "            date_complete.isocalendar().week,\n",
    "            date_complete.dayofyear,\n",
    "            est_weekend,\n",
    "            est_jour_ferie,\n",
    "            nom_jour_ferie,\n",
    "            get_saison(mois)\n",
    "        ]\n",
    "\n",
    "df_dim_date.to_sql(\n",
    "    name='dim_date',\n",
    "    con=engine,\n",
    "    schema='accidents_gold',\n",
    "    if_exists='append',  # ajoute les lignes\n",
    "    index=False,\n",
    "    method=upsert_postgres\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
